{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Phase 2**\n",
        "\n",
        "The process began with getting detailed information and images of artifacts that is called global egyptian musem website. Using BeautifulSoup, we then extracted relevant images saved into a folder, and descriptions of artifacts from websites. Later stored in a structured format in json."
      ],
      "metadata": {
        "id": "1obG-TFJd5f0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CDcjD84CZiCw"
      },
      "outputs": [],
      "source": [
        "#DONT PUT THIS\n",
        "#this code is for testing and printing the html structure of web\n",
        "import urllib.request\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# Base URL\n",
        "base_url = 'https://www.globalegyptianmuseum.org/detail.aspx?id=1'\n",
        "# header\n",
        "headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'}\n",
        "\n",
        "# Parse HTML\n",
        "try:\n",
        "    request = urllib.request.Request(base_url, headers=headers)\n",
        "    response = urllib.request.urlopen(request)\n",
        "    soup = BeautifulSoup(response, \"html.parser\")\n",
        "\n",
        "    # Print the HTML content]\n",
        "    # print(soup.prettify())\n",
        "\n",
        "except urllib.error.HTTPError as e:\n",
        "    print(e)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install requests beautifulsoup4\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uC-TWe7ncOpA",
        "outputId": "7484c29c-8672-4c8c-805c-0712a82fccdd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.31.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (4.12.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2024.6.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4) (2.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import json"
      ],
      "metadata": {
        "id": "q_UEcOJ3OkKj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#download image from url\n",
        "def download_image(url, save_path):\n",
        "    response = requests.get(url)\n",
        "    if response.status_code == 200:\n",
        "        with open(save_path, 'wb') as f:\n",
        "            f.write(response.content)\n",
        "    else:\n",
        "        print(f'Failed to download image from {url}')\n",
        "\n",
        "#scrape data for artifact\n",
        "def scrape_artifact(artifact_id, image_folder):\n",
        "    # Construct the URL\n",
        "    url = f'https://www.globalegyptianmuseum.org/detail.aspx?id={artifact_id}'\n",
        "    response = requests.get(url)\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "    # Extract artifact title\n",
        "    title = soup.find('h1').text.strip()\n",
        "\n",
        "    # Find the image URL\n",
        "    img_tag = soup.find('table', {'id': 'imgtable'}).find('img')\n",
        "    if img_tag and 'src' in img_tag.attrs:\n",
        "        image_relative_url = img_tag['src']\n",
        "        image_url = f'https://www.globalegyptianmuseum.org/{image_relative_url}'\n",
        "    else:\n",
        "        image_url = None\n",
        "        print(f'No image found for artifact ID {artifact_id}')\n",
        "\n",
        "    # Prepare image path\n",
        "    image_name = f'artifact_{artifact_id}.jpg'\n",
        "    image_path = os.path.join(image_folder, image_name)\n",
        "\n",
        "    # Download image if available\n",
        "    if image_url:\n",
        "        download_image(image_url, image_path)\n",
        "\n",
        "    # Extract description\n",
        "    description = soup.find('p', {'id': 'description'}).text.strip()\n",
        "\n",
        "    # Extract details\n",
        "    details = {}\n",
        "    rows = soup.select('table#datatable tr')\n",
        "    for row in rows:\n",
        "        columns = row.find_all('td')\n",
        "        if len(columns) == 2:\n",
        "            key = columns[0].find('h4').text.strip()\n",
        "            value = columns[1].text.strip()\n",
        "            details[key] = value\n",
        "\n",
        "    # Create dictionary\n",
        "    artifact = {\n",
        "        'title': title,\n",
        "        'image': image_path if image_url else None,\n",
        "        'description': description,\n",
        "        'details': details\n",
        "    }\n",
        "    return artifact\n",
        "\n",
        "# Create folder\n",
        "image_folder = 'images'\n",
        "os.makedirs(image_folder, exist_ok=True)\n",
        "\n",
        "# Scrape artifacts from the website\n",
        "artifacts = []\n",
        "start_id = 1\n",
        "end_id = 50 #CHANGE LATER\n",
        "\n",
        "for artifact_id in range(start_id, end_id + 1):\n",
        "    try:\n",
        "        artifact = scrape_artifact(artifact_id, image_folder)\n",
        "        artifacts.append(artifact)\n",
        "    except Exception as e:\n",
        "        print(f'Error scraping artifact ID {artifact_id}: {e}')\n",
        "\n",
        "# Save to JSON file\n",
        "with open('artifacts.json', 'w', encoding='utf-8') as f:\n",
        "    json.dump(artifacts, f, ensure_ascii=False, indent=4)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-imT7QiTcQuS",
        "outputId": "28feb1a8-de2d-4bfd-bb32-631f60c6d52a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully downloaded image: images/artifact_1.jpg\n",
            "Successfully downloaded image: images/artifact_2.jpg\n",
            "Successfully downloaded image: images/artifact_3.jpg\n",
            "Successfully downloaded image: images/artifact_4.jpg\n",
            "Successfully downloaded image: images/artifact_5.jpg\n",
            "Successfully downloaded image: images/artifact_6.jpg\n",
            "Successfully downloaded image: images/artifact_7.jpg\n",
            "Successfully downloaded image: images/artifact_8.jpg\n",
            "Successfully downloaded image: images/artifact_9.jpg\n",
            "Successfully downloaded image: images/artifact_10.jpg\n",
            "Successfully downloaded image: images/artifact_11.jpg\n",
            "Successfully downloaded image: images/artifact_12.jpg\n",
            "Successfully downloaded image: images/artifact_13.jpg\n",
            "Successfully downloaded image: images/artifact_14.jpg\n",
            "Successfully downloaded image: images/artifact_15.jpg\n",
            "Successfully downloaded image: images/artifact_16.jpg\n",
            "Successfully downloaded image: images/artifact_17.jpg\n",
            "Successfully downloaded image: images/artifact_18.jpg\n",
            "Successfully downloaded image: images/artifact_19.jpg\n",
            "Successfully downloaded image: images/artifact_20.jpg\n",
            "Successfully downloaded image: images/artifact_21.jpg\n",
            "Successfully downloaded image: images/artifact_22.jpg\n",
            "Successfully downloaded image: images/artifact_23.jpg\n",
            "Successfully downloaded image: images/artifact_24.jpg\n",
            "Successfully downloaded image: images/artifact_25.jpg\n",
            "Successfully downloaded image: images/artifact_26.jpg\n",
            "Successfully downloaded image: images/artifact_27.jpg\n",
            "Successfully downloaded image: images/artifact_28.jpg\n",
            "Successfully downloaded image: images/artifact_29.jpg\n",
            "Successfully downloaded image: images/artifact_30.jpg\n",
            "Successfully downloaded image: images/artifact_31.jpg\n",
            "Successfully downloaded image: images/artifact_32.jpg\n",
            "Successfully downloaded image: images/artifact_33.jpg\n",
            "Successfully downloaded image: images/artifact_34.jpg\n",
            "Successfully downloaded image: images/artifact_35.jpg\n",
            "Successfully downloaded image: images/artifact_36.jpg\n",
            "Successfully downloaded image: images/artifact_37.jpg\n",
            "Successfully downloaded image: images/artifact_38.jpg\n",
            "Successfully downloaded image: images/artifact_39.jpg\n",
            "Successfully downloaded image: images/artifact_40.jpg\n",
            "Successfully downloaded image: images/artifact_41.jpg\n",
            "Successfully downloaded image: images/artifact_42.jpg\n",
            "Successfully downloaded image: images/artifact_43.jpg\n",
            "Successfully downloaded image: images/artifact_44.jpg\n",
            "Successfully downloaded image: images/artifact_45.jpg\n",
            "Successfully downloaded image: images/artifact_46.jpg\n",
            "Successfully downloaded image: images/artifact_47.jpg\n",
            "Successfully downloaded image: images/artifact_48.jpg\n",
            "Successfully downloaded image: images/artifact_49.jpg\n",
            "Successfully downloaded image: images/artifact_50.jpg\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Phase 3**\n",
        "\n",
        "Used nlp to generate enhanced information for each artifact by using spacy with a summary of key named entities identified."
      ],
      "metadata": {
        "id": "z6oZuuiaflOM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "# Ensure the SpaCy model is installed\n",
        "try:\n",
        "    nlp = spacy.load(\"en_core_web_sm\")\n",
        "except OSError:\n",
        "    print(\"Downloading SpaCy 'en_core_web_sm' model...\")\n",
        "    from spacy.cli import download\n",
        "\n",
        "    download(\"en_core_web_sm\")\n",
        "    nlp = spacy.load(\"en_core_web_sm\")"
      ],
      "metadata": {
        "id": "p2mbJj5SfDow"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#load json file\n",
        "def load_json(file_path):\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        return json.load(f)\n",
        "\n",
        "#Enhance description\n",
        "def generate_description(description, nlp):\n",
        "    doc = nlp(description)\n",
        "\n",
        "    # Collect named entities\n",
        "    entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
        "    # Generate single sentence characteristics(CHANGE)\n",
        "    entity_summary = \", \".join([f\"{text} ({label})\" for text, label in entities])\n",
        "    enhanced_description = f\"{description.strip()} {entity_summary}.\"\n",
        "\n",
        "    return enhanced_description\n",
        "\n",
        "# Remove duplicates\n",
        "def remove_duplicates(artifacts):\n",
        "    unique_titles = set()\n",
        "    unique_artifacts = []\n",
        "\n",
        "    for artifact in artifacts:\n",
        "        title = artifact.get('title', '')\n",
        "        if title not in unique_titles:\n",
        "            unique_titles.add(title)\n",
        "            unique_artifacts.append(artifact)\n",
        "    return unique_artifacts\n",
        "\n",
        "#update description for each artifacts\n",
        "def update_artifact_descriptions(artifacts, nlp):\n",
        "    for artifact in artifacts:\n",
        "        original_description = artifact.get('description', '')\n",
        "        #Enhance the original description using gen_desc func\n",
        "        enhanced_description = generate_description(original_description, nlp)\n",
        "        artifact['enhanced_description'] = enhanced_description\n",
        "    return artifacts\n",
        "\n",
        "#save new enhanced desc into a new json dile\n",
        "def save_json(data, file_path):\n",
        "    with open(file_path, 'w', encoding='utf-8') as f:\n",
        "        json.dump(data, f, ensure_ascii=False, indent=4)\n",
        "\n",
        "#input and output files\n",
        "input_file = 'artifacts.json'\n",
        "output_file = 'artifacts_FinalData.json'\n",
        "\n",
        "# call functions\n",
        "artifacts = load_json(input_file)\n",
        "# remove_duplicates = remove_duplicates(artifacts)\n",
        "updated_artifacts = update_artifact_descriptions(artifacts, nlp)\n",
        "save_json(updated_artifacts, output_file)"
      ],
      "metadata": {
        "id": "WPdsYJ4rgbP9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Phase 5**\n"
      ],
      "metadata": {
        "id": "A5pVtwHPCxRT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow keras numpy scikit-learn matplotlib\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lJmQCFbO31EI",
        "outputId": "d0ecf830-e4fa-4425-b46e-d3a2012a1623"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.15.0)\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.10/dist-packages (2.15.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.25.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.7.1)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.5.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.9.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.12.1)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.37.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.64.1)\n",
            "Requirement already satisfied: tensorboard<2.16,>=2.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.0)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.53.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.5)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.43.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (1.2.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.31.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.0.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (5.3.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2024.6.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow) (2.1.5)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.6.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (3.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.applications.vgg16 import VGG16, preprocess_input\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.models import Model\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import cv2"
      ],
      "metadata": {
        "id": "dNYBL8zmGHBk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#https://medium.com/scrapehero/exploring-image-similarity-approaches-in-python-b8ca0a3ed5a3\n",
        "def load_and_preprocess_image(img_path):\n",
        "    img = cv2.imread(img_path)\n",
        "    # Convert to RGB\n",
        "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "    return img\n",
        "\n",
        "def compute_histogram(img):\n",
        "    hist = cv2.calcHist([img], [0, 1, 2], None, [8, 8, 8], [0, 256, 0, 256, 0, 256])\n",
        "    hist = cv2.normalize(hist, hist).flatten()\n",
        "    return hist\n",
        "\n",
        "# Load and compute their histograms\n",
        "def load_images_and_histograms(image_folder):\n",
        "    image_paths = []\n",
        "    histograms = []\n",
        "\n",
        "    for img_file in os.listdir(image_folder):\n",
        "        if img_file.endswith('.jpg'):\n",
        "            img_path = os.path.join(image_folder, img_file)\n",
        "            img = load_and_preprocess_image(img_path)\n",
        "            hist = compute_histogram(img)\n",
        "            image_paths.append(img_path)\n",
        "            histograms.append(hist)\n",
        "\n",
        "    return image_paths, histograms\n",
        "\n",
        "#simple histogram comparison to find similarity\n",
        "def find_top_similar_images(input_histogram, dataset_histograms, top_n=3):\n",
        "    distances = []\n",
        "    for hist in dataset_histograms:\n",
        "        distance = cv2.compareHist(np.array(input_histogram, dtype=np.float32), np.array(hist, dtype=np.float32), cv2.HISTCMP_BHATTACHARYYA)  # Compute distance\n",
        "        distances.append(distance)\n",
        "    top_indices = np.argsort(distances)[:top_n]\n",
        "    return top_indices\n",
        "\n",
        "# Save images to a folder\n",
        "def save_images(image_paths, save_folder):\n",
        "    if not os.path.exists(save_folder):\n",
        "        os.makedirs(save_folder)\n",
        "    for i, img_path in enumerate(image_paths):\n",
        "        img = cv2.imread(img_path)\n",
        "        save_path = os.path.join(save_folder, f\"image_{i+1}.jpg\")\n",
        "        cv2.imwrite(save_path, img)\n",
        "        print(f\"Saved image: {save_path}\")\n",
        "\n",
        "image_folder = 'images'\n",
        "save_folder = 'sim_images'\n",
        "input_image_path = 'images/artifact_1.jpg'\n",
        "\n",
        "# Load dataset images\n",
        "image_paths, histograms = load_images_and_histograms(image_folder)\n",
        "\n",
        "# Load and preprocess the input image\n",
        "input_image = load_and_preprocess_image(input_image_path)\n",
        "input_histogram = compute_histogram(input_image)\n",
        "\n",
        "# Find the top 3 similar images\n",
        "top_indices = find_top_similar_images(input_histogram, histograms)\n",
        "\n",
        "# Save the selected images\n",
        "top_similar_images = [image_paths[i] for i in top_indices]\n",
        "save_images(top_similar_images, save_folder)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mjt56dSwN9wh",
        "outputId": "8d5f2cd9-49ab-4a42-85f5-bacdb088d918"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved image: sim_images/image_1.jpg\n",
            "Saved image: sim_images/image_2.jpg\n",
            "Saved image: sim_images/image_3.jpg\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # https://www.kaggle.com/code/vincentman0403/find-similar-fruit-images-by-vgg\n",
        "# #load and preprocess one image for vgg\n",
        "# def load_and_preprocess_image(img_path, target_size=(224, 224)):\n",
        "#     img = image.load_img(img_path, target_size=target_size)\n",
        "#     img_data = image.img_to_array(img)\n",
        "#     img_data = np.expand_dims(img_data, axis=0)\n",
        "#     img_data = preprocess_input(img_data)\n",
        "#     return img_data\n",
        "\n",
        "# #load all images from folder\n",
        "# def load_images(image_folder):\n",
        "#     image_paths = []\n",
        "#     images = []\n",
        "#     # Loop through images in folder\n",
        "#     for img in os.listdir(image_folder):\n",
        "#         if img.endswith('.jpg'):\n",
        "#             img_path = os.path.join(image_folder, img)\n",
        "#             image_paths.append(img_path)\n",
        "#             # Load and preprocess the image\n",
        "#             img_data = load_and_preprocess_image(img_path)\n",
        "#             images.append(img_data)\n",
        "#     return images, image_paths\n",
        "\n",
        "# #Extract features from images\n",
        "# def extract_features(images):\n",
        "#     model = VGG16(weights='imagenet', include_top=False, pooling='avg')\n",
        "#     features = [model.predict(img)[0] for img in images]\n",
        "#     return np.array(features)\n",
        "\n",
        "# #cosine similarity between image features.\n",
        "# def compute_cosine_similarity(features):\n",
        "#     return cosine_similarity(features)\n",
        "\n",
        "# # Find the top 3 similar images\n",
        "# def find_top_similar_images(input_features, dataset_features, top_n=3):\n",
        "#     similarities = cosine_similarity([input_features], dataset_features)[0]  # Calculate similarities directly\n",
        "#     top_indices = np.argsort(-similarities)[1:top_n+1]  # Skip the first one which is the input itself\n",
        "#     return top_indices\n",
        "\n",
        "\n",
        "# #Display images directly using PIL.\n",
        "# def display_images_with_pil(image_paths):\n",
        "#     for img_path in image_paths:\n",
        "#         img = Image.open(img_path)\n",
        "#         img.show()\n",
        "\n",
        "# #save images to folder\n",
        "# def save_image(image_paths, save_folder):\n",
        "#     # Create the folder if it doesn't exist\n",
        "#     if not os.path.exists(save_folder):\n",
        "#         os.makedirs(save_folder)\n",
        "#     # Save each image\n",
        "#     for i, img_path in enumerate(image_paths):\n",
        "#         img = Image.open(img_path)\n",
        "#         # Convert to 'RGB' to save as JPEG\n",
        "#         if img.mode not in ('RGB', 'RGBA'):\n",
        "#             img = img.convert('RGB')\n",
        "#         save_path = os.path.join(save_folder, f\"image_{i+1}.jpg\")\n",
        "#         img.save(save_path)\n",
        "#         print(f\"Saved image: {save_path}\")\n",
        "\n",
        "\n",
        "# image_folder = 'images'\n",
        "# save_folder = 'sim_images'\n",
        "# input_image_path = 'images/artifact_1.jpg'\n",
        "\n",
        "# images, image_paths = load_images(image_folder)\n",
        "\n",
        "# features = extract_features(images)\n",
        "\n",
        "# input_image = load_and_preprocess_image(input_image_path)\n",
        "\n",
        "\n",
        "# model = VGG16(weights='imagenet', include_top=False, pooling='avg')\n",
        "# input_features = model.predict(input_image)[0]\n",
        "\n",
        "# top_indices = find_top_similar_images(input_features, features)\n",
        "# top_similar_images = [image_paths[i] for i in top_indices]\n",
        "\n",
        "# save_image(top_similar_images, save_folder)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rez36Z6bJJbP",
        "outputId": "46f2f038-0217-4744-d19e-ed68e5f0c560"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 1s 605ms/step\n",
            "1/1 [==============================] - 0s 476ms/step\n",
            "1/1 [==============================] - 0s 461ms/step\n",
            "1/1 [==============================] - 1s 624ms/step\n",
            "1/1 [==============================] - 1s 855ms/step\n",
            "1/1 [==============================] - 1s 771ms/step\n",
            "1/1 [==============================] - 1s 816ms/step\n",
            "1/1 [==============================] - 1s 810ms/step\n",
            "1/1 [==============================] - 0s 465ms/step\n",
            "1/1 [==============================] - 0s 476ms/step\n",
            "1/1 [==============================] - 0s 464ms/step\n",
            "1/1 [==============================] - 0s 480ms/step\n",
            "1/1 [==============================] - 0s 461ms/step\n",
            "1/1 [==============================] - 0s 482ms/step\n",
            "1/1 [==============================] - 0s 461ms/step\n",
            "1/1 [==============================] - 0s 479ms/step\n",
            "1/1 [==============================] - 0s 455ms/step\n",
            "1/1 [==============================] - 0s 478ms/step\n",
            "1/1 [==============================] - 0s 456ms/step\n",
            "1/1 [==============================] - 0s 478ms/step\n",
            "1/1 [==============================] - 0s 470ms/step\n",
            "1/1 [==============================] - 0s 478ms/step\n",
            "1/1 [==============================] - 0s 463ms/step\n",
            "1/1 [==============================] - 0s 477ms/step\n",
            "1/1 [==============================] - 0s 463ms/step\n",
            "1/1 [==============================] - 0s 480ms/step\n",
            "1/1 [==============================] - 0s 498ms/step\n",
            "1/1 [==============================] - 1s 775ms/step\n",
            "1/1 [==============================] - 1s 824ms/step\n",
            "1/1 [==============================] - 1s 843ms/step\n",
            "1/1 [==============================] - 1s 932ms/step\n",
            "1/1 [==============================] - 1s 601ms/step\n",
            "1/1 [==============================] - 0s 472ms/step\n",
            "1/1 [==============================] - 0s 471ms/step\n",
            "1/1 [==============================] - 1s 574ms/step\n",
            "1/1 [==============================] - 1s 867ms/step\n",
            "1/1 [==============================] - 1s 804ms/step\n",
            "1/1 [==============================] - 1s 847ms/step\n",
            "1/1 [==============================] - 1s 650ms/step\n",
            "1/1 [==============================] - 0s 477ms/step\n",
            "1/1 [==============================] - 0s 462ms/step\n",
            "1/1 [==============================] - 0s 479ms/step\n",
            "1/1 [==============================] - 0s 462ms/step\n",
            "1/1 [==============================] - 0s 475ms/step\n",
            "1/1 [==============================] - 0s 463ms/step\n",
            "1/1 [==============================] - 0s 478ms/step\n",
            "1/1 [==============================] - 0s 463ms/step\n",
            "1/1 [==============================] - 1s 557ms/step\n",
            "1/1 [==============================] - 1s 844ms/step\n",
            "1/1 [==============================] - 1s 811ms/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "Saved image: sim_images/image_1.jpg\n",
            "Saved image: sim_images/image_2.jpg\n",
            "Saved image: sim_images/image_3.jpg\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Phase 6**"
      ],
      "metadata": {
        "id": "6RZ5U4Z7VVLw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#simple chatbot for testing without ui\n",
        "# Load artifact data from JSON file\n",
        "# import json\n",
        "def load_artifact_data(json_path):\n",
        "    with open(json_path, 'r', encoding='utf-8') as f:\n",
        "        artifacts = json.load(f)\n",
        "    return artifacts\n",
        "\n",
        "#  Find and return information about artifacts\n",
        "def find_artifact_info(artifacts, query):\n",
        "    results = []\n",
        "    query_lower = query.lower()\n",
        "    found_titles = set()\n",
        "\n",
        "    for artifact in artifacts:\n",
        "        title = artifact.get('title', '').lower()\n",
        "        description = artifact.get('description', '').lower()\n",
        "        category = artifact.get('details', {}).get('Category', 'No Category')\n",
        "\n",
        "        # Create a unified condition to avoid duplication\n",
        "        if (query_lower in title or query_lower in description) and title not in found_titles:\n",
        "            results.append({\n",
        "                'id': artifact.get('id', 'No ID'),\n",
        "                'title': artifact.get('title', 'No Title'),\n",
        "                'description': artifact.get('description', 'No Description'),\n",
        "                'category': category,\n",
        "                'Present location': artifact.get('details', {}).get('Present location', 'No Location'),\n",
        "                'Inventory number': artifact.get('details', {}).get('Inventory number', 'No Inventory Number'),\n",
        "                'Dating': artifact.get('details', {}).get('Dating', 'No Dating'),\n",
        "                'Archaeological Site': artifact.get('details', {}).get('Archaeological Site', 'No Site'),\n",
        "                'Material': artifact.get('details', {}).get('Material', 'No Material'),\n",
        "                'Technique': artifact.get('details', {}).get('Technique', 'No Technique'),\n",
        "                'Height': artifact.get('details', {}).get('Height', 'No Height'),\n",
        "                'Width': artifact.get('details', {}).get('Width', 'No Width'),\n",
        "                'Depth': artifact.get('details', {}).get('Depth', 'No Depth')\n",
        "            })\n",
        "            found_titles.add(title)\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "#Interact with the chatbot testing\n",
        "def chat_with_bot(artifacts):\n",
        "  #Type 'exit' to end the chat\n",
        "    while True:\n",
        "      # Start the conversation\n",
        "        query = input(\"You: \")\n",
        "\n",
        "        if query.lower() == 'exit':\n",
        "            break\n",
        "\n",
        "        results = find_artifact_info(artifacts, query)\n",
        "\n",
        "        if results:\n",
        "            for info in results:\n",
        "                print(f\"\\n---\")\n",
        "                print(f\"Chatbot: As a historian, I can tell you about {info['title']} (ID: {info['id']}):\")\n",
        "                print(\"Description:\")\n",
        "                print(info['description'])\n",
        "                print(\"Category:\", info['category'])\n",
        "                print(\"Present location:\", info['Present location'])\n",
        "                print(\"Inventory number:\", info['Inventory number'])\n",
        "                print(\"Dating:\", info['Dating'])\n",
        "                print(\"Archaeological Site:\", info['Archaeological Site'])\n",
        "                print(\"Material:\", info['Material'])\n",
        "                print(\"Technique:\", info['Technique'])\n",
        "                print(\"Height:\", info['Height'])\n",
        "                print(\"Width:\", info['Width'])\n",
        "                print(\"Depth:\", info['Depth'])\n",
        "        else:\n",
        "            print(\"Chatbot: I couldn't understand your question.\")\n",
        "            print(\"Please include the exact title of an artifact or the name of a category in your query to know more about it.\")\n",
        "\n",
        "# Load artifact data\n",
        "json_path = 'artifacts_FinalData.json'\n",
        "artifacts = load_artifact_data(json_path)\n",
        "chat_with_bot(artifacts)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ubgI1RzVYXB",
        "outputId": "ff6f3fe3-794f-4313-bd64-e3333ccee93b"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "You: mummy label\n",
            "\n",
            "---\n",
            "Chatbot: As a historian, I can tell you about Mummy label (ID: No ID):\n",
            "Description:\n",
            "From the 2nd century of our era, the Egyptians  produced documents of wood called \"mummy labels\". They contain short religious formulae mentioning the name, affiliation, occupation, age and the date of death of the deceased. This piece in the form of a sign was intended to be fixed to the mummy of Sarapammon, son of Harpalos. The Museum owns a second \"label\" belonging to the brother of Sarapammon (A.1974).\n",
            "Category: MUMMY LABEL\n",
            "Present location: KMKG - MRAH [07/003] BRUSSELS\n",
            "Inventory number: A.1973\n",
            "Dating: ROMAN PERIOD\n",
            "Archaeological Site: TIHNA EL-GABAL\n",
            "Material: WOOD\n",
            "Technique: SCULPTURED; ENGRAVED\n",
            "Height: 10.5 cm\n",
            "Width: No Width\n",
            "Depth: No Depth\n",
            "You: mummy label\n",
            "\n",
            "---\n",
            "Chatbot: As a historian, I can tell you about Mummy label (ID: No ID):\n",
            "Description:\n",
            "From the 2nd century of our era, the Egyptians  produced documents of wood called \"mummy labels\". They contain short religious formulae mentioning the name, affiliation, occupation, age and the date of death of the deceased. This piece in the form of a sign was intended to be fixed to the mummy of Sarapammon, son of Harpalos. The Museum owns a second \"label\" belonging to the brother of Sarapammon (A.1974).\n",
            "Category: MUMMY LABEL\n",
            "Present location: KMKG - MRAH [07/003] BRUSSELS\n",
            "Inventory number: A.1973\n",
            "Dating: ROMAN PERIOD\n",
            "Archaeological Site: TIHNA EL-GABAL\n",
            "Material: WOOD\n",
            "Technique: SCULPTURED; ENGRAVED\n",
            "Height: 10.5 cm\n",
            "Width: No Width\n",
            "Depth: No Depth\n",
            "You: exit\n"
          ]
        }
      ]
    }
  ]
}